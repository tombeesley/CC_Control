@article{beesley2012,
  title = {Investigating Cue Competition in Contextual Cuing of Visual Search.},
  author = {Beesley, Tom and Shanks, David R.},
  date = {2012},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {38},
  number = {3},
  pages = {709--725},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/a0024885},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024885},
  urldate = {2019-08-07},
  abstract = {A fundamental principle of learning is that predictive cues or signals compete with each other to gain control over behavior. Associative and propositional reasoning theories of learning provide radically different accounts of cue competition. Propositional accounts predict that under conditions that do not afford or warrant the use of higher order reasoning processes, cue competition should not be observed. We tested this prediction in 2 contextual cuing experiments, using a visual search task in which patterns of distractor elements predict the location of a target object. Blocking designs were used in which 2 sets of predictive distractors were trained in compound, with 1 set trained independently. There was no evidence of cue competition in either experiment. In fact, in Experiment 2, we found evidence for augmentation of learning. The findings are contrasted with the predictions of an error-driven associative model of contextual cuing (Brady \& Chun, 2007).},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Beesley, Shanks - 2012.pdf}
}

@article{beesley2015b,
  title = {Pre-Exposure of Repeated Search Configurations Facilitates Subsequent Contextual Cuing of Visual Search.},
  author = {Beesley, Tom and Vadillo, Miguel A. and Pearson, Daniel and Shanks, David R.},
  date = {2015},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {41},
  number = {2},
  pages = {348--362},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000033},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xlm0000033},
  urldate = {2019-08-07},
  abstract = {Contextual cuing is the enhancement of visual search when the configuration of distractors has been experienced previously. It has been suggested that contextual cuing relies on associative learning between the distractor locations and the target position. Four experiments examined the effect of pre-exposing configurations of consistent distractors on subsequent contextual cuing. The findings demonstrate a facilitation of subsequent cuing for pre-exposed configurations compared to novel configurations that have not been pre-exposed. This facilitation suggests that learning of repeated visual search patterns involves acquisition of not just distractor–target associations but also associations between distractors within the search context, an effect that is not captured by the Brady and Chun (2007) connectionist model of contextual cuing. We propose a new connectionist model of contextual cuing that learns associations between repeated distractor stimuli, enabling it to predict an effect of pre-exposure on contextual cuing.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Beesley, Vadillo, Pearson et al - 2015.pdf}
}

@article{beesley2016,
  title = {Configural Learning in Contextual Cuing of Visual Search.},
  author = {Beesley, Tom and Vadillo, Miguel A. and Pearson, Daniel and Shanks, David R.},
  date = {2016},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {42},
  number = {8},
  pages = {1173--1185},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/xhp0000185},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xhp0000185},
  urldate = {2019-08-07},
  abstract = {Two experiments were conducted to explore the role of configural representations in contextual cuing of visual search. Repeating patterns of distractors (contexts) were trained incidentally as predictive of the target location. Training participants with repeating contexts of consistent configurations led to stronger contextual cuing than when participants were trained with contexts of inconsistent configurations. Computational simulations with an elemental associative learning model of contextual cuing demonstrated that purely elemental representations could not account for the results. However, a configural model of associative learning was able to simulate the ordinal pattern of data.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Beesley, Vadillo, Pearson et al - 2016.pdf}
}

@article{beesley2018,
  title = {Overt Attention in Contextual Cuing of Visual Search Is Driven by the Attentional Set, but Not by the Predictiveness of Distractors.},
  author = {Beesley, Tom and Hanafi, Gunadi and Vadillo, Miguel A. and family=Shanks, given=David. R., given-i={{David}}R and Livesey, Evan J.},
  date = {2018-05},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {44},
  number = {5},
  pages = {707--721},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000467},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xlm0000467},
  urldate = {2019-08-07},
  abstract = {Two experiments examined biases in selective attention during contextual cuing of visual search. When participants were instructed to search for a target of a particular color, overt attention (as measured by the location of fixations) was biased strongly toward distractors presented in that same color. However, when participants searched for targets that could be presented in 1 of 2 possible colors, overt attention was not biased between the different distractors, regardless of whether these distractors predicted the location of the target (repeating) or did not (randomly arranged). These data suggest that selective attention in visual search is guided only by the demands of the target detection task (the attentional set) and not by the predictive validity of the distractor elements.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Beesley, Hanafi, Vadillo et al - 2018.pdf}
}

@article{brady2007,
  title = {Spatial Constraints on Learning in Visual Search: {{Modeling}} Contextual Cuing.},
  shorttitle = {Spatial Constraints on Learning in Visual Search},
  author = {Brady, Timothy F. and Chun, Marvin M.},
  date = {2007},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {4},
  pages = {798--815},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.33.4.798},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.33.4.798},
  urldate = {2019-08-07},
  abstract = {Predictive visual context facilitates visual search, a benefit termed contextual cuing (M. M. Chun \& Y. Jiang, 1998). In the original task, search arrays were repeated across blocks such that the spatial configuration (context) of all of the distractors in a display predicted an embedded target location. The authors modeled existing results using a connectionist architecture and then designed new behavioral experiments to test the model’s assumptions. The modeling and behavioral results indicate that learning may be restricted to the local context even when the entire configuration is predictive of target location. Local learning constrains how much guidance is produced by contextual cuing. The modeling and new data also demonstrate that local learning requires that the local context maintain its location in the overall global context.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Brady, Chun - 2007.pdf}
}

@article{chun1998,
  title = {Contextual {{Cueing}}: {{Implicit Learning}} and {{Memory}} of {{Visual Context Guides Spatial Attention}}},
  shorttitle = {Contextual {{Cueing}}},
  author = {Chun, Marvin M. and Jiang, Yuhong},
  date = {1998-06},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {36},
  number = {1},
  pages = {28--71},
  issn = {00100285},
  doi = {10.1006/cogp.1998.0681},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028598906818},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Chun, Jiang - 1998.pdf}
}

@article{chun2000a,
  title = {On the {{Functional Role}} of {{Implicit Visual Memory}} for the {{Adaptive Deployment}} of {{Attention Across Scenes}}},
  author = {Chun, Marvin M. and Nakayama, Ken},
  date = {2000-01},
  journaltitle = {Visual Cognition},
  shortjournal = {Visual Cognition},
  volume = {7},
  number = {1-3},
  pages = {65--81},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/135062800394685},
  url = {http://www.tandfonline.com/doi/abs/10.1080/135062800394685},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Chun, Nakayama - 2000.pdf}
}

@article{colagiuri2016,
  title = {Contextual Cuing as a Form of Nonconscious Learning: {{Theoretical}} and Empirical Analysis in Large and Very Large Samples},
  shorttitle = {Contextual Cuing as a Form of Nonconscious Learning},
  author = {Colagiuri, Ben and Livesey, E. J.},
  date = {2016-12},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  number = {6},
  pages = {1996--2009},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1063-0},
  url = {http://link.springer.com/10.3758/s13423-016-1063-0},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Colagiuri, Livesey - 2016.pdf}
}

@article{endo2004,
  title = {Selective Learning of Spatial Configuration and Object Identity in Visual Search},
  author = {Endo, Nobutaka and Takeda, Yuji},
  date = {2004-02},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {66},
  number = {2},
  pages = {293--302},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03194880},
  url = {http://www.springerlink.com/index/10.3758/BF03194880},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Endo, Takeda - 2004.pdf}
}

@article{geyer2021,
  title = {Why {{Are Acquired Search-Guiding Context Memories Resistant}} to {{Updating}}?},
  author = {Geyer, Thomas and Seitz, Werner and Zinchenko, Artyom and Müller, Hermann J. and Conci, Markus},
  date = {2021-03-01},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {12},
  eprint = {33732200},
  eprinttype = {pmid},
  pages = {650245},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.650245},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7956950/},
  urldate = {2023-07-10},
  abstract = {Looking for goal-relevant objects in our various environments is one of the most ubiquitous tasks the human visual system has to accomplish (Wolfe, ). Visual search is guided by a number of separable selective-attention mechanisms that can be categorized as bottom-up driven – guidance by salient physical properties of the current stimuli – or top-down controlled – guidance by observers' “online” knowledge of search-critical object properties (e.g., Liesefeld and Müller, ). In addition, observers' expectations based on past experience also play also a significant role in goal-directed visual selection. Because sensory environments are typically stable, it is beneficial for the visual system to extract and learn the environmental regularities that are predictive of (the location of) the target stimulus. This perspective article is concerned with one of these predictive mechanisms: statistical context learning of consistent spatial patterns of target and distractor items in visual search. We review recent studies on context learning and its adaptability to incorporate consistent changes, with the aim to provide new directions to the study of processes involved in the acquisition of search-guiding context memories and their adaptation to consistent contextual changes – from a three-pronged, psychological, computational, and neurobiological perspective.},
  pmcid = {PMC7956950},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Geyer, Seitz, Zinchenko et al - 2021.pdf}
}

@article{kunar2007,
  title = {Does Contextual Cuing Guide the Deployment of Attention?},
  author = {Kunar, Melina A. and Flusberg, Stephen and Horowitz, Todd S. and Wolfe, Jeremy M.},
  date = {2007},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {4},
  pages = {816--828},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.33.4.816},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.33.4.816},
  urldate = {2019-08-07},
  abstract = {Contextual cuing experiments show that when displays are repeated, reaction times to find a target decrease over time even when observers are not aware of the repetition. It has been thought that the context of the display guides attention to the target. The authors tested this hypothesis by comparing the effects of guidance in a standard search task with the effects of contextual cuing. First, in standard search, an improvement in guidance causes search slopes (derived from Reaction Time ϫ Set Size functions) to decrease. In contrast, the authors found that search slopes in contextual cuing did not become more efficient over time (Experiment 1). Second, when guidance was optimal (e.g., in easy feature search), they still found a small but reliable contextual cuing effect (Experiments 2a and 2b), suggesting that other factors, such as response selection, contribute to the effect. Experiment 3 supported this hypothesis by showing that the contextual cuing effect disappeared when the authors added interference to the response selection process. Overall, the data suggest that the relationship between guidance and contextual cuing is weak and that response selection can account for part of the effect.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Kunar, Flusberg, Horowitz et al - 2007.pdf}
}

@article{kunar2014,
  title = {A Configural Dominant Account of Contextual Cueing: {{Configural}} Cues Are Stronger than Colour Cues},
  shorttitle = {A Configural Dominant Account of Contextual Cueing},
  author = {Kunar, Melina A. and John, Rebecca and Sweetman, Hollie},
  date = {2014},
  journaltitle = {Quarterly Journal of Experimental Psychology (2006)},
  shortjournal = {Q J Exp Psychol (Hove)},
  volume = {67},
  number = {7},
  eprint = {24199842},
  eprinttype = {pmid},
  pages = {1366--1382},
  issn = {1747-0226},
  doi = {10.1080/17470218.2013.863373},
  abstract = {Previous work has shown that reaction times to find a target in displays that have been repeated are faster than those for displays that have never been seen before. This learning effect, termed "contextual cueing" (CC), has been shown using contexts such as the configuration of the distractors in the display and the background colour. However, it is not clear how these two contexts interact to facilitate search. We investigated this here by comparing the strengths of these two cues when they appeared together. In Experiment 1, participants searched for a target that was cued by both colour and distractor configural cues, compared with when the target was only predicted by configural information. The results showed that the addition of a colour cue did not increase contextual cueing. In Experiment 2, participants searched for a target that was cued by both colour and distractor configuration compared with when the target was only cued by colour. The results showed that adding a predictive configural cue led to a stronger CC benefit. Experiments 3 and 4 tested the disruptive effects of removing either a learned colour cue or a learned configural cue and whether there was cue competition when colour and configural cues were presented together. Removing the configural cue was more disruptive to CC than removing colour, and configural learning was shown to overshadow the learning of colour cues. The data support a configural dominant account of CC, where configural cues act as the stronger cue in comparison to colour when they are presented together.},
  langid = {english},
  keywords = {Adult,Analysis of Variance,Attention,Color Perception,Colour,Configuration,Contextual cueing,Cues,Female,Humans,Learning,Male,Middle Aged,Pattern Recognition Visual,Reaction Time,Space Perception,Visual attention,Visual search,Young Adult},
  file = {C:\Users\beesleyt\Zotero\storage\IQ9JCVKR\Kunar, John, Sweetman - 2014.pdf}
}

@article{luque2017b,
  title = {Testing the Controllability of Contextual Cuing of Visual Search},
  author = {Luque, David and Vadillo, Miguel A. and Lopez, Francisco J. and Alonso, Rafael and Shanks, David R.},
  date = {2017-05},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {7},
  number = {1},
  pages = {39645},
  issn = {2045-2322},
  doi = {10.1038/srep39645},
  url = {http://www.nature.com/articles/srep39645},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Luque, Vadillo, Lopez et al - 2017.pdf}
}

@article{luque2021,
  title = {Contextual Cuing of Visual Search Does Not Guide Attention Automatically in the Presence of Top-down Goals},
  author = {Luque, David and Beesley, Tom and Molinero, Sara and Vadillo, Miguel A.},
  date = {2021},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {47},
  number = {8},
  pages = {1080--1090},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1277},
  doi = {10.1037/xhp0000930},
  abstract = {Visual search is faster when it occurs within repeated displays, a phenomenon known as contextual cuing (CC). CC has been explained as the result of an automatic orientation of attention toward a target item driven by learned distractor-target associations. In 3 experiments we tested the specific hypothesis that CC is an automatic process of attentional guidance. Participants first searched for a T target in a standard CC procedure. Then, they experienced the same repeated configurations (with the T still present), but now searched for a Y target that was positioned either in a location on the same, or on a different side, from the old T target. Results suggested that there was no interference caused by the old T-target: target search was not affected by the relative positions of the T and Y. Instead, we found a general facilitation in search times for repeated configurations (Experiments 1 and 2). This main effect disappeared when the need for visual search was eliminated in Experiment 3 using a “feature search task”. These results suggest that repeated sets of distractors did not trigger an uncontrollable response toward the position of the T; instead, CC was produced by perceptual learning processes. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Automatism,Contextual Cues,Perceptual Learning,Visual Attention,Visual Displays,Visual Search},
  file = {C:\Users\beesleyt\Zotero\storage\WFVWI6QZ\Luque et al. - 2021.pdf}
}

@article{makovski2011,
  title = {Investigating the {{Role}} of {{Response}} in {{Spatial Context Learning}}},
  author = {Makovski, Tal and Jiang, Yuhong V.},
  date = {2011-08},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  shortjournal = {Quarterly Journal of Experimental Psychology},
  volume = {64},
  number = {8},
  pages = {1563--1579},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470218.2011.564291},
  url = {http://journals.sagepub.com/doi/10.1080/17470218.2011.564291},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Makovski, Jiang - 2011.pdf}
}

@article{makovski2017,
  title = {Learning “{{What}}” and “{{Where}}” in {{Visual Search}}},
  author = {Makovski, Tal},
  date = {2017},
  journaltitle = {Japanese Psychological Research},
  volume = {59},
  number = {2},
  pages = {133--143},
  issn = {1468-5884},
  doi = {10.1111/jpr.12146},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jpr.12146},
  urldate = {2023-07-04},
  abstract = {Visual search is facilitated when observers search through repeated displays. This effect, termed contextual cueing (CC), reflects the exceptional ability of our cognitive system to utilize regularities embedded in the environment. Recent studies that tested visual search with real-world objects found that CC takes place even in heterogeneous search displays, but only when the identities (“what”) and locations (“where”) of the objects are both repeated. The purpose of the current study was to test whether the repetition of both “what” and “where” is not only necessary but also sufficient for CC. Consistent with previous results, Experiment 1 found robust CC when both the “what” and “where” information were repeated, and further revealed that the effect was not modulated by the number of search items. In contrast, Experiment 2 showed that the repetition of both objects’ identities and locations did not benefit the search when the two were not bound together. CC was also absent in Experiment 3, where the objects’ identities and locations were repeated together, however, target locations varied randomly. Together these results suggest that CC with real-world objects is robust, but critically depends on “what” and “where” binding as well as context-target associations.},
  langid = {english},
  keywords = {contextual cueing,visual learning,visual search,what and where},
  file = {C:\Users\beesleyt\Zotero\storage\FQA2HBI2\Makovski - 2017.pdf}
}

@article{makovski2018,
  title = {Meaning in Learning: {{Contextual}} Cueing Relies on Objects' Visual Features and Not on Objects' Meaning},
  shorttitle = {Meaning in Learning},
  author = {Makovski, Tal},
  date = {2018-01},
  journaltitle = {Memory \& Cognition},
  shortjournal = {Mem. Cogn.},
  volume = {46},
  number = {1},
  pages = {58--67},
  publisher = {Springer},
  location = {New York},
  issn = {0090-502X},
  doi = {10.3758/s13421-017-0745-9},
  url = {https://link.springer.com/article/10.3758/s13421-017-0745-9},
  urldate = {2023-07-04},
  abstract = {People easily learn regularities embedded in the environment and utilize them to facilitate visual search. Using images of real-world objects, it has been recently shown that this learning, termed contextual cueing (CC), occurs even in complex, heterogeneous environments, but only when the same distractors are repeated at the same locations. Yet it is not clear what exactly is being learned under these conditions: the visual features of the objects or their meaning. In this study, Experiment 1 demonstrated that meaning is not necessary for this type of learning, as a similar pattern of results was found even when the objects' meaning was largely removed. Experiments 2 and 3 showed that after learning meaningful objects, CC was not diminished by a manipulation that distorted the objects' meaning but preserved most of their visual properties. By contrast, CC was eliminated when the learned objects were replaced with different category exemplars that preserved the objects' meaning but altered their visual properties. Together, these data strongly suggest that the acquired context that facilitates real-world objects search relies primarily on the visual properties and the spatial locations of the objects, but not on their meaning.},
  langid = {english},
  keywords = {configuration,constraints,Contextual cueing,eye-movements,implicit,long-term-memory,real-world scenes,regularities,search,semantic information,Semantics,spatial   context,Visual learning,Visual search},
  annotation = {WOS:000419719900005},
  file = {C:\Users\beesleyt\Zotero\storage\WQ3ABYTG\Makovski - 2018.pdf}
}

@article{manginelli2009,
  title = {Misleading Contextual Cues: {{How}} Do They Affect Visual Search?},
  shorttitle = {Misleading Contextual Cues},
  author = {Manginelli, Angela A. and Pollmann, Stefan},
  date = {2009-03},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  volume = {73},
  number = {2},
  pages = {212--221},
  issn = {0340-0727, 1430-2772},
  doi = {10.1007/s00426-008-0211-1},
  url = {http://link.springer.com/10.1007/s00426-008-0211-1},
  urldate = {2019-08-07},
  abstract = {Contextual cueing occurs when repetitions of the distractor conWguration are implicitly learned. This implicit learning leads to faster search times in repeated displays. Here, we investigated how search adapts to a change of the target location in old displays from a consistent location in the learning phase to a consistent new location in the transfer phase. In agreement with the literature, contextual cueing was accompanied by fewer Wxations, a more eYcient scan path and, speciWcally, an earlier onset of a monotonic gaze approach phase towards the target location in repeated displays. When the repeated context was no longer predictive of the old target location, search times and number of Wxations for old displays increased to the level of novel displays. Along with this, scan paths for old and new displays became equally eYcient. After the target location change, there was a bias of exploration towards the old target location, which soon disappeared. Thus, change of implicitly learned spatial relations between target and distractor conWguration eliminated the advantageous eVects of contextual cueing, but did not lead to a lasting impairment of search in repeated displays relative to novel displays.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Manginelli, Pollmann - 2009.pdf}
}

@article{moors2006,
  title = {Automaticity: {{A Theoretical}} and {{Conceptual Analysis}}.},
  shorttitle = {Automaticity},
  author = {Moors, Agnes and De Houwer, Jan},
  date = {2006},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {132},
  number = {2},
  pages = {297--326},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.132.2.297},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.132.2.297},
  urldate = {2019-08-07},
  abstract = {Several theoretical views of automaticity are discussed. Most of these suggest that automaticity should be diagnosed by looking at the presence of features such as unintentional, uncontrolled/uncontrollable, goal independent, autonomous, purely stimulus driven, unconscious, efficient, and fast. Contemporary views further suggest that these features should be investigated separately. The authors examine whether features of automaticity can be disentangled on a conceptual level, because only then is the separate investigation of them worth the effort. They conclude that the conceptual analysis of features is to a large extent feasible. Not all researchers agree with this position, however. The authors show that assumptions of overlap among features are determined by the other researchers’ views of automaticity and by the models they endorse for information processing in general.},
  langid = {english},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Moors, De Houwer - 2006.pdf;C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Moors, De Houwer - 22.pdf}
}

@article{olson2002,
  title = {Perceptual Constraints on Implicit Learning of Spatial Context},
  author = {Olson, Ingrid R. and Chun, Marvin M.},
  date = {2002-04},
  journaltitle = {Visual Cognition},
  shortjournal = {Visual Cognition},
  volume = {9},
  number = {3},
  pages = {273--302},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506280042000162},
  url = {http://www.tandfonline.com/doi/abs/10.1080/13506280042000162},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Olson, Chun - 2002.pdf}
}

@article{renaux2017,
  title = {Role of Spatial Contiguity in Sensory Preconditioning with Humans},
  author = {Renaux, Charlotte and Rivière, Vinca and Craddock, Paul and Miller, Ralph R.},
  date = {2017-09-01},
  journaltitle = {Behavioural Processes},
  shortjournal = {Behavioural Processes},
  volume = {142},
  pages = {141--145},
  issn = {0376-6357},
  doi = {10.1016/j.beproc.2017.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0376635717301845},
  urldate = {2023-08-23},
  abstract = {The present study demonstrates the contribution of spatial contiguity in the formation of associations between two neutral stimuli. Using human participants, we used visual conditioned stimuli (CSs) in a sensory preconditioning design in which simultaneous CS2-CS1 pairings and CS4-CS3 pairings were interspersed during Phase 1, followed by sequential CS1-US+ (i.e., CS1-pleasant US) and CS3-US-− (i.e., CS3-unpleasant US) pairings during Phase 2. The conditioned response was a shift in the gaze of the participants to the location where the US+ (i.e., short video clip) appeared. Distances between CS2 and CS1 and between CS4 and CS3 were manipulated. Our results showed a greater response to CS2 when the Phase 1 stimuli were adjacent rather than separated by 100 pixels. Implications for the role of spatial contiguity in associative learning are discussed.},
  keywords = {Eye-tracking,Humans,Sensory preconditioning,Spatial contiguity},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Renaux, Rivière, Craddock et al - 2017.pdf}
}

@article{rouder2017,
  title = {Bayesian Analysis of Factorial Designs},
  author = {Rouder, Jeffrey N. and Morey, Richard D. and Verhagen, Josine and Swagman, April R. and Wagenmakers, Eric-Jan},
  date = {2017-06},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  series = {Bayesian {{Data Analysis}} - {{Part I}}},
  volume = {22},
  number = {2},
  pages = {304--321},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/met0000057},
  url = {https://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2016-28700-001&site=ehost-live&authtype=ip,shib&user=s1523151},
  urldate = {2022-03-10},
  abstract = {This article provides a Bayes factor approach to multiway analysis of variance (ANOVA) that allows researchers to state graded evidence for effects or invariances as determined by the data. ANOVA is conceptualized as a hierarchical model where levels are clustered within factors. The development is comprehensive in that it includes Bayes factors for fixed and random effects and for within-subjects, between-subjects, and mixed designs. Different model construction and comparison strategies are discussed, and an example is provided. We show how Bayes factors may be computed with BayesFactor package in R and with the JASP statistical package. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Analysis of Variance,ANOVA,Bayes factors,Bayes Theorem,Bayesian analysis,Bayesian Analysis,Humans,model selection,Models Statistical,Research Design,Statistical Analysis,Statistical Probability},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Rouder, Morey, Verhagen et al - 2017.pdf}
}

@article{sewell2018,
  title = {Response Time Modeling Reveals Multiple Contextual Cuing Mechanisms},
  author = {Sewell, David K. and Colagiuri, Ben and Livesey, Evan J.},
  date = {2018-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  number = {5},
  pages = {1644--1665},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1364-y},
  url = {https://doi.org/10.3758/s13423-017-1364-y},
  urldate = {2023-06-30},
  abstract = {Contextual cuing refers to a response time (RT) benefit that occurs when observers search through displays that have been repeated over the course of an experiment. Although it is generally agreed that contextual cuing arises via an associative learning mechanism, there is uncertainty about the type(s) of process(es) that allow learning to influence RT. We contrast two leading accounts of the contextual cuing effect that differ in terms of the general process that is credited with producing the effect. The first, the expedited search account, attributes the cuing effect to an increase in the speed with which the target is acquired. The second, the decision threshold account, attributes the cuing effect to a reduction in the response threshold used by observers when making a subsequent decision about the target (e.g., judging its orientation). We use the diffusion model to contrast the quantitative predictions of these two accounts at the level of individual observers. Our use of the diffusion model allows us to also explore a novel decision-level locus of the cuing effect based on perceptual learning. This novel account attributes the RT benefit to a perceptual learning process that increases the quality of information used to drive the decision process. Our results reveal both individual differences in the process(es) involved in contextual cuing but also identify several striking regularities across observers. We find strong support for both the decision threshold account as well as the novel perceptual learning account. We find relatively weak support for the expedited search account.},
  langid = {english},
  keywords = {Computational modeling,Contextual cuing,Diffusion model,Response times,Visual search},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Sewell, Colagiuri, Livesey - 2018.pdf}
}

@article{sisk2019,
  title = {Mechanisms of Contextual Cueing: {{A}} Tutorial Review},
  shorttitle = {Mechanisms of Contextual Cueing},
  author = {Sisk, Caitlin A. and Remington, Roger W. and Jiang, Yuhong V.},
  date = {2019-11-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  volume = {81},
  number = {8},
  pages = {2571--2589},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01832-2},
  url = {https://doi.org/10.3758/s13414-019-01832-2},
  urldate = {2022-03-16},
  abstract = {Repeated contexts yield faster response time in visual search, compared with novel contexts. This effect is known as contextual cueing. Despite extensive study over the past two decades, there remains a spirited debate over whether repeated displays expedite search before the target is found (early locus) or facilitate response after the target is found (late locus). Here, we provide a tutorial review of contextual cueing, with a focus on assessing the locus of the effect. We evaluate the evidence from psychophysics, EEG, and eye tracking. Existing studies support an early locus of contextual cueing, consistent with attentional guidance accounts. Evidence for a late locus exists, though it is less conclusive. Existing literature also highlights a distinction between habit-guided attention learned through experience and changes in spatial priority driven by task goals and stimulus salience.},
  langid = {english},
  keywords = {_tablet},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Sisk, Remington, Jiang - 2019.pdf}
}

@article{smyth2008,
  title = {Awareness in Contextual Cuing with Extended and Concurrent Explicit Tests},
  author = {Smyth, Andrea C. and Shanks, David R.},
  date = {2008-03},
  journaltitle = {Memory \& Cognition},
  shortjournal = {Memory \& Cognition},
  volume = {36},
  number = {2},
  pages = {403--415},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/MC.36.2.403},
  url = {http://www.springerlink.com/index/10.3758/MC.36.2.403},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Smyth, Shanks - 2008.pdf}
}

@article{tseng2004,
  title = {Oculomotor Correlates of Context-Guided Learning in Visual Search},
  author = {Tseng, Yuan-Chi and Li, Chiang-Shan Ray},
  date = {2004-11},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {66},
  number = {8},
  pages = {1363--1378},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03195004},
  url = {http://www.springerlink.com/index/10.3758/BF03195004},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Tseng, Li - 2004.pdf}
}

@article{vadillo2016,
  title = {Underpowered Samples, False Negatives, and Unconscious Learning},
  author = {Vadillo, Miguel A. and Konstantinidis, Emmanouil and Shanks, David R.},
  date = {2016-02},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  number = {1},
  pages = {87--102},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-015-0892-6},
  url = {http://link.springer.com/10.3758/s13423-015-0892-6},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Vadillo, Konstantinidis, Shanks - 2016.pdf}
}

@article{vickery2005,
  title = {Setting up the Target Template in Visual Search},
  author = {Vickery, Timothy J. and King, Li-Wei and Jiang, Yuhong},
  date = {2005-02-09},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {5},
  number = {1},
  pages = {8},
  issn = {1534-7362},
  doi = {10.1167/5.1.8},
  url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/5.1.8},
  urldate = {2019-08-07},
  abstract = {Top-down knowledge about the target is essential in visual search. It biases visual attention to information that matches the target-defining criteria. Extensive research in the past has examined visual search when the target is defined by fixed criteria throughout the experiment, with few studies investigating how subjects set up the target. To address this issue, we conducted five experiments using random polygons and real-world objects, allowing the target criteria to change from trial to trial. On each trial, subjects first see a cue informing them about the target, followed 200-1000 ms later by the search array. We find that when the cue matches the target exactly, search speed increases and the slope of response time–set size function decreases. Deviations from the exact match in size or orientation slow down search speed, although they lead to faster speed compared with a neutral cue or a semantic cue. We conclude that the template set-up process uses detailed visual information, rather than schematic or semantic information, to find the target.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Vickery, King, Jiang - 2005.pdf}
}

@article{vo2012,
  title = {When Does Repeated Search in Scenes Involve Memory? {{Looking}} at versus Looking for Objects in Scenes},
  shorttitle = {When Does Repeated Search in Scenes Involve Memory?},
  author = {Võ, Melissa L.-H. and Wolfe, Jeremy M.},
  date = {2012-02},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {38},
  number = {1},
  eprint = {21688939},
  eprinttype = {pmid},
  pages = {23--41},
  issn = {1939-1277},
  doi = {10.1037/a0024147},
  abstract = {One might assume that familiarity with a scene or previous encounters with objects embedded in a scene would benefit subsequent search for those items. However, in a series of experiments we show that this is not the case: When participants were asked to subsequently search for multiple objects in the same scene, search performance remained essentially unchanged over the course of searches despite increasing scene familiarity. Similarly, looking at target objects during previews, which included letter search, 30 seconds of free viewing, or even 30 seconds of memorizing a scene, also did not benefit search for the same objects later on. However, when the same object was searched for again memory for the previous search was capable of producing very substantial speeding of search despite many different intervening searches. This was especially the case when the previous search engagement had been active rather than supported by a cue. While these search benefits speak to the strength of memory-guided search when the same search target is repeated, the lack of memory guidance during initial object searches-despite previous encounters with the target objects-demonstrates the dominance of guidance by generic scene knowledge in real-world search.},
  langid = {english},
  pmcid = {PMC3969238},
  keywords = {Adult,Eye Movements,Female,Humans,Learning,Male,Memory Episodic,Neuropsychological Tests,Space Perception,Visual Perception}
}

@article{wetzels2011a,
  title = {Statistical {{Evidence}} in {{Experimental Psychology}}: {{An Empirical Comparison Using}} 855 t {{Tests}}},
  shorttitle = {Statistical {{Evidence}} in {{Experimental Psychology}}},
  author = {Wetzels, Ruud and Matzke, Dora and Lee, Michael D. and Rouder, Jeffrey N. and Iverson, Geoffrey J. and Wagenmakers, Eric-Jan},
  date = {2011-05-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {6},
  number = {3},
  pages = {291--298},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691611406923},
  url = {https://doi.org/10.1177/1745691611406923},
  urldate = {2023-08-08},
  abstract = {Statistical inference in psychology has traditionally relied heavily on p-value significance testing. This approach to drawing conclusions from data, however, has been widely criticized, and two types of remedies have been advocated. The first proposal is to supplement p values with complementary measures of evidence, such as effect sizes. The second is to replace inference with Bayesian measures of evidence, such as the Bayes factor. The authors provide a practical comparison of p values, effect sizes, and default Bayes factors as measures of statistical evidence, using 855 recently published t tests in psychology. The comparison yields two main results. First, although p values and default Bayes factors almost always agree about what hypothesis is better supported by the data, the measures often disagree about the strength of this support; for 70\% of the data sets for which the p value falls between .01 and .05, the default Bayes factor indicates that the evidence is only anecdotal. Second, effect sizes can provide additional evidence to p values and default Bayes factors. The authors conclude that the Bayesian approach is comparatively prudent, preventing researchers from overestimating the evidence in favor of an effect.},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Wetzels, Matzke, Lee et al - 2011.pdf}
}

@article{zellin2013,
  title = {Statistical Learning in the Past Modulates Contextual Cueing in the Future},
  author = {Zellin, M. and family=Muhlenen, given=A., prefix=von, useprefix=true and Muller, H. J. and Conci, M.},
  date = {2013-07-23},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {13},
  number = {3},
  pages = {19--19},
  issn = {1534-7362},
  doi = {10.1167/13.3.19},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/13.3.19},
  urldate = {2019-08-07},
  langid = {english},
  file = {C:\Users\beesleyt\OneDrive - Lancaster University\Desktop PC\Papers - Zotero PDF Library\Zellin, von Muhlenen, Muller et al - 2013.pdf}
}
