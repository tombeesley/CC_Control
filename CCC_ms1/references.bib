@Article{shanks1985,
  title = {Forward and {{Backward Blocking}} in {{Human Contingency Judgement}}},
  author = {David R. Shanks},
  date = {1985-02},
  journaltitle = {The Quarterly Journal of Experimental Psychology Section B},
  shortjournal = {The Quarterly Journal of Experimental Psychology Section B},
  volume = {37},
  pages = {1--21},
  issn = {0272-4995, 1464-1321},
  doi = {10.1080/14640748508402082},
  url = {http://journals.sagepub.com/doi/10.1080/14640748508402082},
  urldate = {2019-08-07},
  issue = {1b},
  langid = {english},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Shanks - 1985.pdf},
}
@Article{vickery2005,
  title = {Setting up the Target Template in Visual Search},
  author = {Timothy J. Vickery and Li-Wei King and Yuhong Jiang},
  date = {2005-02-09},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {5},
  number = {1},
  pages = {8},
  issn = {1534-7362},
  doi = {10.1167/5.1.8},
  url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/5.1.8},
  urldate = {2019-08-07},
  abstract = {Top-down knowledge about the target is essential in visual search. It biases visual attention to information that matches the target-defining criteria. Extensive research in the past has examined visual search when the target is defined by fixed criteria throughout the experiment, with few studies investigating how subjects set up the target. To address this issue, we conducted five experiments using random polygons and real-world objects, allowing the target criteria to change from trial to trial. On each trial, subjects first see a cue informing them about the target, followed 200-1000 ms later by the search array. We find that when the cue matches the target exactly, search speed increases and the slope of response time–set size function decreases. Deviations from the exact match in size or orientation slow down search speed, although they lead to faster speed compared with a neutral cue or a semantic cue. We conclude that the template set-up process uses detailed visual information, rather than schematic or semantic information, to find the target.},
  langid = {english},
  file = {C\:\\Users\\beesleyt\\OneDrive - Lancaster University\\Desktop PC\\Papers - Zotero PDF Library\\Vickery, King, Jiang - 2005.pdf},
}

@Article{vo2012,
  title = {When Does Repeated Search in Scenes Involve Memory? {{Looking}} at versus Looking for Objects in Scenes},
  shorttitle = {When Does Repeated Search in Scenes Involve Memory?},
  author = {Melissa L.-H. V{\~o} and Jeremy M. Wolfe},
  date = {2012-02},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {38},
  number = {1},
  eprint = {21688939},
  eprinttype = {pmid},
  pages = {23--41},
  issn = {1939-1277},
  doi = {10.1037/a0024147},
  abstract = {One might assume that familiarity with a scene or previous encounters with objects embedded in a scene would benefit subsequent search for those items. However, in a series of experiments we show that this is not the case: When participants were asked to subsequently search for multiple objects in the same scene, search performance remained essentially unchanged over the course of searches despite increasing scene familiarity. Similarly, looking at target objects during previews, which included letter search, 30 seconds of free viewing, or even 30 seconds of memorizing a scene, also did not benefit search for the same objects later on. However, when the same object was searched for again memory for the previous search was capable of producing very substantial speeding of search despite many different intervening searches. This was especially the case when the previous search engagement had been active rather than supported by a cue. While these search benefits speak to the strength of memory-guided search when the same search target is repeated, the lack of memory guidance during initial object searches-despite previous encounters with the target objects-demonstrates the dominance of guidance by generic scene knowledge in real-world search.},
  langid = {english},
  pmcid = {PMC3969238},
  keywords = {Adult,Eye Movements,Female,Humans,Learning,Male,{Memory, Episodic},Neuropsychological Tests,Space Perception,Visual Perception},
}
@Article{makovski2017,
  title = {Learning “{{What}}” and “{{Where}}” in {{Visual Search}}},
  author = {Tal Makovski},
  date = {2017},
  journaltitle = {Japanese Psychological Research},
  volume = {59},
  number = {2},
  pages = {133--143},
  issn = {1468-5884},
  doi = {10.1111/jpr.12146},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jpr.12146},
  urldate = {2023-07-04},
  abstract = {Visual search is facilitated when observers search through repeated displays. This effect, termed contextual cueing (CC), reflects the exceptional ability of our cognitive system to utilize regularities embedded in the environment. Recent studies that tested visual search with real-world objects found that CC takes place even in heterogeneous search displays, but only when the identities (“what”) and locations (“where”) of the objects are both repeated. The purpose of the current study was to test whether the repetition of both “what” and “where” is not only necessary but also sufficient for CC. Consistent with previous results, Experiment 1 found robust CC when both the “what” and “where” information were repeated, and further revealed that the effect was not modulated by the number of search items. In contrast, Experiment 2 showed that the repetition of both objects’ identities and locations did not benefit the search when the two were not bound together. CC was also absent in Experiment 3, where the objects’ identities and locations were repeated together, however, target locations varied randomly. Together these results suggest that CC with real-world objects is robust, but critically depends on “what” and “where” binding as well as context-target associations.},
  langid = {english},
  keywords = {contextual cueing,visual learning,visual search,what and where},
  file = {C\:\\Users\\beesleyt\\Zotero\\storage\\FQA2HBI2\\Makovski - 2017.pdf},
}

@Article{makovski2018,
  title = {Meaning in Learning: {{Contextual}} Cueing Relies on Objects' Visual Features and Not on Objects' Meaning},
  shorttitle = {Meaning in Learning},
  author = {Tal Makovski},
  date = {2018-01},
  journaltitle = {Memory \& Cognition},
  shortjournal = {Mem. Cogn.},
  volume = {46},
  number = {1},
  pages = {58--67},
  publisher = {{Springer}},
  location = {{New York}},
  issn = {0090-502X},
  doi = {10.3758/s13421-017-0745-9},
  url = {https://link.springer.com/article/10.3758/s13421-017-0745-9},
  urldate = {2023-07-04},
  abstract = {People easily learn regularities embedded in the environment and utilize them to facilitate visual search. Using images of real-world objects, it has been recently shown that this learning, termed contextual cueing (CC), occurs even in complex, heterogeneous environments, but only when the same distractors are repeated at the same locations. Yet it is not clear what exactly is being learned under these conditions: the visual features of the objects or their meaning. In this study, Experiment 1 demonstrated that meaning is not necessary for this type of learning, as a similar pattern of results was found even when the objects' meaning was largely removed. Experiments 2 and 3 showed that after learning meaningful objects, CC was not diminished by a manipulation that distorted the objects' meaning but preserved most of their visual properties. By contrast, CC was eliminated when the learned objects were replaced with different category exemplars that preserved the objects' meaning but altered their visual properties. Together, these data strongly suggest that the acquired context that facilitates real-world objects search relies primarily on the visual properties and the spatial locations of the objects, but not on their meaning.},
  langid = {english},
  keywords = {configuration,constraints,Contextual cueing,eye-movements,implicit,long-term-memory,real-world scenes,regularities,search,semantic information,Semantics,spatial   context,Visual learning,Visual search},
  annotation = {WOS:000419719900005},
  file = {C\:\\Users\\beesleyt\\Zotero\\storage\\WQ3ABYTG\\Makovski - 2018.pdf},
}
